theory verifythis_2021_shearsort_ShearSort_sort_columnqtvc
  imports "NTP4Verif.NTP4Verif" "Why3STD.Ref_Ref" "Why3STD.int_NumOf" "Why3STD.int_Sum" "Why3STD.map_MapExt" "Why3STD.map_MapPermut" "Why3STD.matrix_Matrix"
begin
consts column :: "'a matrix \<Rightarrow> int \<Rightarrow> int \<Rightarrow> 'a"
axiomatization where column'def:   "column m j i = elts m i j"
  for m :: "'a matrix"
  and j :: "int"
  and i :: "int"
consts moccf :: "'a \<Rightarrow> (int \<Rightarrow> int \<Rightarrow> 'a) \<Rightarrow> int \<Rightarrow> int \<Rightarrow> int"
axiomatization where moccf'def:   "moccf x e c i = int (map_occ x (e i) (0 :: int) c)"
  for x :: "'a"
  and e :: "int \<Rightarrow> int \<Rightarrow> 'a"
  and c :: "int"
  and i :: "int"
definition mocc :: "'a \<Rightarrow> (int \<Rightarrow> int \<Rightarrow> 'a) \<Rightarrow> int \<Rightarrow> int \<Rightarrow> int"
  where "mocc x e r c = sum (moccf x e c) (0 :: int) r" for x e r c
consts compose :: "('b \<Rightarrow> 'c) \<Rightarrow> ('a \<Rightarrow> 'b) \<Rightarrow> 'a \<Rightarrow> 'c"
axiomatization where compose'def:   "compose g f x = g (f x)"
  for g :: "'b \<Rightarrow> 'c"
  and f :: "'a \<Rightarrow> 'b"
  and x :: "'a"
theorem sort_column'vc:
  fixes j :: "int"
  fixes m :: "int matrix"
  assumes fact0: "(0 :: int) \<le> j"
  assumes fact1: "j < columns m"
  shows "let o1 :: int = rows m in (0 :: int) \<le> o1 \<and> (\<forall>(a :: int list). (\<forall>(i :: int). (0 :: int) \<le> i \<and> i < o1 \<longrightarrow> a ! nat i = (0 :: int)) \<and> int (length a) = o1 \<longrightarrow> (let o2 :: int = rows m - (1 :: int) in ((0 :: int) \<le> o2 + (1 :: int) \<longrightarrow> (\<forall>(k :: int). (0 :: int) \<le> k \<and> k < (0 :: int) \<longrightarrow> (nth a o nat) k = elts m k j) \<and> (\<forall>(a1 :: int list). length a1 = length a \<longrightarrow> (\<forall>(i :: int). ((0 :: int) \<le> i \<and> i \<le> o2) \<and> (\<forall>(k :: int). (0 :: int) \<le> k \<and> k < i \<longrightarrow> (nth a1 o nat) k = elts m k j) \<longrightarrow> valid_index m i j \<and> (let o3 :: int = elts m i j in ((0 :: int) \<le> i \<and> i < int (length a1)) \<and> (length (a1[nat i := o3]) = length a1 \<longrightarrow> nth (a1[nat i := o3]) o nat = (nth a1 o nat)(i := o3) \<longrightarrow> (\<forall>(k :: int). (0 :: int) \<le> k \<and> k < i + (1 :: int) \<longrightarrow> (nth (a1[nat i := o3]) o nat) k = elts m k j)))) \<and> ((\<forall>(k :: int). (0 :: int) \<le> k \<and> k < o2 + (1 :: int) \<longrightarrow> (nth a1 o nat) k = elts m k j) \<longrightarrow> (\<forall>(a2 :: int list). length a2 = length a1 \<longrightarrow> (\<forall>(i :: int) (j1 :: int). (0 :: int) \<le> i \<and> i \<le> j1 \<and> j1 < int (length a2) \<longrightarrow> a2 ! nat i \<le> a2 ! nat j1) \<and> permut (nth a2 o nat) (nth a1 o nat) (0 :: int) (int (length a2)) \<longrightarrow> (let o3 :: int = rows m - (1 :: int) in ((0 :: int) \<le> o3 + (1 :: int) \<longrightarrow> (\<forall>(k :: int). (0 :: int) \<le> k \<and> k < (0 :: int) \<longrightarrow> (nth a2 o nat) k = elts m k j) \<and> (\<forall>(m1 :: int matrix). rows m1 = rows m \<and> columns m1 = columns m \<longrightarrow> (\<forall>(i :: int). ((0 :: int) \<le> i \<and> i \<le> o3) \<and> (\<forall>(k :: int). (0 :: int) \<le> k \<and> k < i \<longrightarrow> (nth a2 o nat) k = elts m1 k j) \<and> (\<forall>(k :: int) (l :: int). ((0 :: int) \<le> k \<and> k < rows m1) \<and> ((0 :: int) \<le> l \<and> l < columns m1) \<and> \<not>l = j \<longrightarrow> elts m1 k l = elts m k l) \<longrightarrow> ((0 :: int) \<le> i \<and> i < int (length a2)) \<and> valid_index m1 i j \<and> (\<forall>(m2 :: int matrix). rows m2 = rows m1 \<and> columns m2 = columns m1 \<longrightarrow> elts m2 = (elts m1)(i := (elts m1 i)(j := a2 ! nat i)) \<longrightarrow> (\<forall>(k :: int). (0 :: int) \<le> k \<and> k < i + (1 :: int) \<longrightarrow> (nth a2 o nat) k = elts m2 k j) \<and> (\<forall>(k :: int) (l :: int). ((0 :: int) \<le> k \<and> k < rows m2) \<and> ((0 :: int) \<le> l \<and> l < columns m2) \<and> \<not>l = j \<longrightarrow> elts m2 k l = elts m k l))) \<and> ((\<forall>(k :: int). (0 :: int) \<le> k \<and> k < o3 + (1 :: int) \<longrightarrow> (nth a2 o nat) k = elts m1 k j) \<and> (\<forall>(k :: int) (l :: int). ((0 :: int) \<le> k \<and> k < rows m1) \<and> ((0 :: int) \<le> l \<and> l < columns m1) \<and> \<not>l = j \<longrightarrow> elts m1 k l = elts m k l) \<longrightarrow> (\<forall>(i :: int) (k :: int). (0 :: int) \<le> i \<and> i < rows m1 \<and> ((0 :: int) \<le> k \<and> k < columns m1) \<and> \<not>k = j \<longrightarrow> elts m1 i k = elts m i k) \<and> (\<forall>(i :: int) (k :: int). (0 :: int) \<le> i \<and> i \<le> k \<and> k < rows m1 \<longrightarrow> elts m1 i j \<le> elts m1 k j) \<and> permut (column m1 j) (column m j) (0 :: int) (rows m1)))) \<and> (o3 + (1 :: int) < (0 :: int) \<longrightarrow> (\<forall>(i :: int) (k :: int). (0 :: int) \<le> i \<and> i \<le> k \<and> k < rows m \<longrightarrow> elts m i j \<le> elts m k j) \<and> permut (column m j) (column m j) (0 :: int) (rows m))))))) \<and> (o2 + (1 :: int) < (0 :: int) \<longrightarrow> (\<forall>(a1 :: int list). length a1 = length a \<longrightarrow> (\<forall>(i :: int) (j1 :: int). (0 :: int) \<le> i \<and> i \<le> j1 \<and> j1 < int (length a1) \<longrightarrow> a1 ! nat i \<le> a1 ! nat j1) \<and> permut (nth a1 o nat) (nth a o nat) (0 :: int) (int (length a1)) \<longrightarrow> (let o3 :: int = rows m - (1 :: int) in ((0 :: int) \<le> o3 + (1 :: int) \<longrightarrow> (\<forall>(k :: int). (0 :: int) \<le> k \<and> k < (0 :: int) \<longrightarrow> (nth a1 o nat) k = elts m k j) \<and> (\<forall>(m1 :: int matrix). rows m1 = rows m \<and> columns m1 = columns m \<longrightarrow> (\<forall>(i :: int). ((0 :: int) \<le> i \<and> i \<le> o3) \<and> (\<forall>(k :: int). (0 :: int) \<le> k \<and> k < i \<longrightarrow> (nth a1 o nat) k = elts m1 k j) \<and> (\<forall>(k :: int) (l :: int). ((0 :: int) \<le> k \<and> k < rows m1) \<and> ((0 :: int) \<le> l \<and> l < columns m1) \<and> \<not>l = j \<longrightarrow> elts m1 k l = elts m k l) \<longrightarrow> ((0 :: int) \<le> i \<and> i < int (length a1)) \<and> valid_index m1 i j \<and> (\<forall>(m2 :: int matrix). rows m2 = rows m1 \<and> columns m2 = columns m1 \<longrightarrow> elts m2 = (elts m1)(i := (elts m1 i)(j := a1 ! nat i)) \<longrightarrow> (\<forall>(k :: int). (0 :: int) \<le> k \<and> k < i + (1 :: int) \<longrightarrow> (nth a1 o nat) k = elts m2 k j) \<and> (\<forall>(k :: int) (l :: int). ((0 :: int) \<le> k \<and> k < rows m2) \<and> ((0 :: int) \<le> l \<and> l < columns m2) \<and> \<not>l = j \<longrightarrow> elts m2 k l = elts m k l))) \<and> ((\<forall>(k :: int). (0 :: int) \<le> k \<and> k < o3 + (1 :: int) \<longrightarrow> (nth a1 o nat) k = elts m1 k j) \<and> (\<forall>(k :: int) (l :: int). ((0 :: int) \<le> k \<and> k < rows m1) \<and> ((0 :: int) \<le> l \<and> l < columns m1) \<and> \<not>l = j \<longrightarrow> elts m1 k l = elts m k l) \<longrightarrow> (\<forall>(i :: int) (k :: int). (0 :: int) \<le> i \<and> i < rows m1 \<and> ((0 :: int) \<le> k \<and> k < columns m1) \<and> \<not>k = j \<longrightarrow> elts m1 i k = elts m i k) \<and> (\<forall>(i :: int) (k :: int). (0 :: int) \<le> i \<and> i \<le> k \<and> k < rows m1 \<longrightarrow> elts m1 i j \<le> elts m1 k j) \<and> permut (column m1 j) (column m j) (0 :: int) (rows m1)))) \<and> (o3 + (1 :: int) < (0 :: int) \<longrightarrow> (\<forall>(i :: int) (k :: int). (0 :: int) \<le> i \<and> i \<le> k \<and> k < rows m \<longrightarrow> elts m i j \<le> elts m k j) \<and> permut (column m j) (column m j) (0 :: int) (rows m)))))))"
  sorry
end
